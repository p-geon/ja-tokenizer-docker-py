{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09fdebe3",
   "metadata": {},
   "source": [
    "# Tokenizer Mecab +NEologd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae31ade",
   "metadata": {},
   "source": [
    "## 初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88f8306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_neologd = os.environ.get('DIR_NEOLOGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fccd02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d26005",
   "metadata": {},
   "source": [
    "## Tokenizerクラス作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a802285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self, dir_dict: str):\n",
    "        self.dir_dict = dir_dict\n",
    "\n",
    "        self.tokenizer = self.init_neologd()\n",
    "        self.tokenizer.parse('')\n",
    "\n",
    "\n",
    "    def __call__(self, text: str) -> Tuple[str, str]:\n",
    "        nodes = self.get_nodes(text)\n",
    "        ret =  self.parse_nodes(nodes)\n",
    "        return ret\n",
    "\n",
    "\n",
    "    def init_neologd(self) -> MeCab.Tagger:\n",
    "        tokenizer = MeCab.Tagger(f'-r/dev/null -d{self.dir_dict}') # <- https://github.com/SamuraiT/mecab-python3#common-issues\n",
    "        return tokenizer\n",
    "\n",
    "    \n",
    "    def get_nodes(self, sentence: str) -> MeCab.Node:\n",
    "        return self.tokenizer.parseToNode(sentence)\n",
    "\n",
    "\n",
    "    def parse_nodes(self, nodes: MeCab.Node) -> Tuple[str, str]:\n",
    "        words, parts = [], []\n",
    "        \n",
    "        while nodes:\n",
    "            if(nodes.feature.split(\",\")[6] == '*'):\n",
    "                word = nodes.surface\n",
    "            else:\n",
    "                word = nodes.feature.split(\",\")[6]\n",
    "\n",
    "            words.append(word)\n",
    "            parts.append(nodes.feature.split(\",\")[0])\n",
    "\n",
    "            nodes = nodes.next\n",
    "        return words, parts\n",
    "    \n",
    "    \n",
    "tokenizer = Tokenizer(dir_dict=dir_neologd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "437af5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tokenized_result(sentence: str) -> None:\n",
    "    ws, ps = tokenizer.__call__(sentence)\n",
    "    for w, p in zip(ws, ps):\n",
    "        print(f'{w}\\t{p}')\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04402116",
   "metadata": {},
   "source": [
    "## 実行\n",
    "\n",
    "`sentence`引数に渡した文を分かち書きした結果を表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42472623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBOS/EOS\n",
      "ピジョン\t名詞\n",
      "と\t助詞\n",
      "ジョン・レノン\t名詞\n",
      "が\t助詞\n",
      "融合\t名詞\n",
      "する\t動詞\n",
      "て\t助詞\n",
      "ピジョンレノン\t名詞\n",
      "と\t助詞\n",
      "成る\t動詞\n",
      "た\t助動詞\n",
      "。\t記号\n",
      "\tBOS/EOS\n",
      "------------------------------\n",
      "\tBOS/EOS\n",
      "ぷるぷる\t副詞\n",
      "プルキンエ細胞\t名詞\n",
      "\tBOS/EOS\n",
      "------------------------------\n",
      "\tBOS/EOS\n",
      "まずは\t接続詞\n",
      "Tehu\t名詞\n",
      "を\t助詞\n",
      "オフチョベット\t名詞\n",
      "する\t動詞\n",
      "ます\t助動詞\n",
      "。\t記号\n",
      "そして\t接続詞\n",
      "オフチョベット\t名詞\n",
      "する\t動詞\n",
      "た\t助動詞\n",
      "Tehu\t名詞\n",
      "を\t助詞\n",
      "マブガッド\t名詞\n",
      "する\t動詞\n",
      "て\t助詞\n",
      "リット\t名詞\n",
      "を\t助詞\n",
      "作る\t動詞\n",
      "の\t助詞\n",
      "。\t記号\n",
      "そして\t接続詞\n",
      "発酵\t名詞\n",
      "する\t動詞\n",
      "せる\t動詞\n",
      "た\t助動詞\n",
      "リット\t名詞\n",
      "に\t助詞\n",
      "アブシィト\t名詞\n",
      "を\t助詞\n",
      "加える\t動詞\n",
      "て\t助詞\n",
      "混ぜる\t動詞\n",
      "ます\t助動詞\n",
      "。\t記号\n",
      "あと\t名詞\n",
      "は\t助詞\n",
      "焼く\t動詞\n",
      "だけ\t助詞\n",
      "よ\t助詞\n",
      "\tBOS/EOS\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "check_tokenized_result(sentence=\"ピジョンとジョン・レノンが融合してピジョンレノンと成った。\")\n",
    "check_tokenized_result(sentence=\"プルプルプルキンエ細胞\")\n",
    "check_tokenized_result(sentence=\"\"\"まずはテフをオフチョベットします。\n",
    "    そしてオフチョベットしたテフをマブガッドしてリットを作るの。\n",
    "    そして発酵させたリットにアブシィトを加えて混ぜます。\n",
    "    あとは焼くだけよ\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
